VisTOS – Visual Fields for Lightweight Time Series Transformers in Earth Observation Semantic Segmentation, Johannnes Keller

How to use pretraining and fine-tuning:

    Pretraining:    

        1)  Get the original pretraining dataset, following the instructions froms Tseng et al. (2024) from: https://github.com/nasaharvest/presto
        2)  Define the TRAIN_URL and VAL_URL variables according to the Google Cloud bucket, where the data resides. Authenticate to Google Cloud.
        3)  Define environment, BATCH_SIZE, TRAIN_DATA_LENGTH, and  VAL_DATA_LENGTH in params. 
        4)  Start from the console: $ python main.py [model_type] pretrain [visual_field_size]
        The parameter model_type can be "att" for the attention-based spatial encoding model or "conv" for the convolution-based spatial encoding model, the parameter "visual_field_size" must be defined and can be 1 (for Presto), 3 for a 3x3-pixel visual field, and 5 for a 5x5-pixel visual field in combination with the attention-based architecture ("att") and 3, 5, and 7 for a 7x7-pixel visual field for the convolution-based architecture ("conv").
        5) The model is cached automatically in the directory "output/cache", where it is saved as a
        dictionary containing the training progress. After pretraining, the model is saved as .pth file in "output". From there, it is loaded for fine-tuning.
    
    Fine-tuning:

        PASTIS-R dataset:

        1)  Obtain the PASTIS-R dataset from: https://zenodo.org/records/5735646, change the constant
        P_PATH in params accordingly. 
        2)  The class weights for inverse frequency weightig can be started by uncommenting the main() call at the end of the pastis_dataset file. Start calculation then via: 
        $ python -m dataset.pastis_dataset
        3)  The pretrained model must reside in "output" as a .pth file following the pretraining naming conventions, with the same model type and the same visual field size you want to train. Otherwise a previously fine-tuned cached model is preferably loaded from "output/cache" if available. 
        4) Start the fine-tuning, after defining the corresponding parameters in params, with:
        $ python main.py [model_type] finetune pastis [visual_field_size]. The parameter model_type can be "att" for the attention-based spatial encoding model or "conv" for the convolution-based spatial encoding model, the parameter "visual_field_size" must be defined and can be 1 (for Presto), 3 for a 3x3-pixel visual field, and 5 for a 5x5-pixel visual field in combination with the attention-based architecture ("att") and 3, 5, and 7 for a 7x7-pixel visual field for the convolution-based architecture ("conv").
        5) The model is cached automatically in the directory "output/cache", where it is saved as a
        dictionary containing the training progress.

        CDDS dataset:

        1) Obtain the dataset, as indicated below, change the constant CDDS_PATH in params accordingly. Further, the paths to the split .csv files must be defined: CDDS_TRAIN_PATH, CDDS_VAL_PATH, CDDS_TEST_PATH.
        2)  The class weights for inverse frequency weightig can be started by uncommenting the main() call at the end of the cdds_dataset file. Start calculation then via: 
        $ python -m dataset.cdds_dataset
        3)  The pretrained model must reside in "output" as a .pth file following the pretraining naming conventions, with the same model type and the same visual field size you want to train. Otherwise a previously fine-tuned cached model is preferably loaded from "output/cache" if available. 
        4) Start the fine-tuning, after defining the corresponding parameters in params, with:
        $ python main.py [model_type] finetune cdds [visual_field_size]. The parameter model_type can be "att" for the attention-based spatial encoding model or "conv" for the convolution-based spatial encoding model, the parameter "visual_field_size" must be defined and can be 1 (for Presto), 3 for a 3x3-pixel visual field, and 5 for a 5x5-pixel visual field in combination with the attention-based architecture ("att") and 3, 5, and 7 for a 7x7-pixel visual field for the convolution-based architecture ("conv").
        5) The model is cached automatically in the directory "output/cache", where it is saved as a
        dictionary containing the training progress.



How to use CDDS dataset pipeline:

    The pipeline can be started from the module "dataops/cdds_pipeline". 

    1)  Download the original dataset "my_examples_landsat_final.zip" by Debus et al. (2024) from: https://zenodo.org/records/8325259 and update CAMEROON_LABELS_PTH in the params module accordingly. Also update CAMEROON_PATH in params.

    2)  For running the script, several prerequisites are necessary: a Google account, an Google Earth Engine account, Google Drive API credentials. Uncomment the last line in the cdds_pipeline module and call main() to run the download and processing: $ python -m dataops.cdds_pipeline. Queue size and desired number of samples are set via the arguments of run_acquisition().


Credits: 

    The code of the modules dataset, model, finetunig, utils, and pretraining is based on:

        "Presto: Lightweight, Pre-trained Transformers for Remote Sensing Timeseries",
        Gabriel Tseng, Ruben Cartuyvels, Ivan Zvonkov, Mirali Purohit, David Rolnick, Hannah Kerner (2024),
        arXiv: https://arxiv.org/abs/2304.14065,
        GitHub: https://github.com/nasaharvest/presto/

    The code of the module pastis_r_dataset is based on:

        "Multi-Modal Temporal Attention Models for Crop Mapping from Satellite Time Series", 
        Vivien Sainte Fare Garnot, Loïc Landrieu, Nesrine Chehata (2021),
        arXiv: https://arxiv.org/abs/2112.07558,
        GitHub: https://github.com/VSainteuf/pastis-benchmark/blob/main/code/dataloader.py

    The CDDS dataset is based on: 

        "Labelled dataset to classify direct deforestation drivers in Cameroon [Data set]",
        Debus, A., Beauchamp, E., Acworth, J., Ewolo, A., Kamga, J., Verhegghen, A., Zébazé, C., & Lines, E. R. (2024).
        In Scientific Data (1.00, Vol.11, Number 1, p.564). 
        Zenodo: https://doi.org/10.5281/zenodo.8325259
